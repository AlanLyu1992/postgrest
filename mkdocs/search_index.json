{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nPostgREST is a standalone web server that turns your database directly into a RESTful API. The structural constraints and permissions in the database determine the API endpoints and operations.\n\n\nThis guide explains how to install the software and provides practical examples of its use. You'll learn how to build a fast, versioned, secure API and how to deploy it to production.\n\n\nThe project has a friendly and growing community. Here are some ways to get help or get involved:\n\n\n\n\nThe project \nchat room\n\n\nReport or search \nissues\n\n\n\n\nMotivation\n\n\nUsing PostgREST is an alternative to manual CRUD programming. Custom API servers suffer problems. Writing business logic often duplicates, ignores or hobbles database structure. Object-relational mapping is a leaky abstraction leading to slow imperative code. The PostgREST philosophy establishes a single declarative source of truth: the data itself.\n\n\nDeclarative Programming\n\n\nIt's easier to ask Postgres to join data for you and let its query planner figure out the details than to loop through rows yourself. It's easier to assign permissions to db objects than to add guards in controllers. (This is especially true for cascading permissions in data dependencies.) It's easier set constraints than to litter code with sanity checks.\n\n\nLeakproof Abstraction\n\n\nThere is no ORM involved. Creating new views happens in SQL with known performance implications. A database administrator can now create an API from scratch with no custom programming. \n\n\nEmbracing the Relational Model\n\n\nIn 1970 E. F. Codd criticized the then-dominant hierarchical model of databases in his article \nA Relational Model of Data for Large Shared Data Banks\n. Reading the article reveals a striking similarity between hierarchical databases and nested http routes. With PostgREST we attempt to use flexible filtering and embedding rather than nested routes.\n\n\nOne Thing Well\n\n\nPostgREST has a focused scope. It works well with other tools like Nginx. This forces you to cleanly separate the data-centric CRUD operations from other concerns. Use a collection of sharp tools rather than building a big ball of mud.\n\n\nShared Improvements\n\n\nAs with any open source project, we all gain from features and fixes in the tool. It's more beneficial than improvements locked inextricably within custom codebases.\n\n\nMyths\n\n\nYou have to make tons of stored procs and triggers\n\n\nModern PostgreSQL features like auto-updatable views and computed columns make this mostly unnecessary. Triggers do play a part, but generally not for irksome boilerplate. When they are required triggers are preferable to ad-hoc app code anyway, since the former work reliably for any codepath.\n\n\nExposing the database destroys encapsulation\n\n\nPostgREST does versioning through database schemas. This allows you to expose tables and views without making the app brittle. Underlying tables can be superseded and hidden behind public facing views. The chapter about versioning shows how to do this.\n\n\nConventions\n\n\nThis guide contains highlighted notes and tangential information interspersed with the text.\n\n\n\n    \nDesign Consideration\n\n\n    \nContains history which informed the current design. Sometimes it discusses unavoidable tradeoffs or a point of theory.\n\n\n\n\n\n\n    \nInvitation to Contribute\n\n\n    \nPoints out things we know we want to add or improve. They might give you ideas for ways to contribute to the project.\n\n\n\n\n\n\n    \nDeprecation Warning\n\n\n    \nAlerts you to features which will be removed in the next major (breaking) release.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "PostgREST is a standalone web server that turns your database directly into a RESTful API. The structural constraints and permissions in the database determine the API endpoints and operations.  This guide explains how to install the software and provides practical examples of its use. You'll learn how to build a fast, versioned, secure API and how to deploy it to production.  The project has a friendly and growing community. Here are some ways to get help or get involved:   The project  chat room  Report or search  issues   Motivation  Using PostgREST is an alternative to manual CRUD programming. Custom API servers suffer problems. Writing business logic often duplicates, ignores or hobbles database structure. Object-relational mapping is a leaky abstraction leading to slow imperative code. The PostgREST philosophy establishes a single declarative source of truth: the data itself.  Declarative Programming  It's easier to ask Postgres to join data for you and let its query planner figure out the details than to loop through rows yourself. It's easier to assign permissions to db objects than to add guards in controllers. (This is especially true for cascading permissions in data dependencies.) It's easier set constraints than to litter code with sanity checks.  Leakproof Abstraction  There is no ORM involved. Creating new views happens in SQL with known performance implications. A database administrator can now create an API from scratch with no custom programming.   Embracing the Relational Model  In 1970 E. F. Codd criticized the then-dominant hierarchical model of databases in his article  A Relational Model of Data for Large Shared Data Banks . Reading the article reveals a striking similarity between hierarchical databases and nested http routes. With PostgREST we attempt to use flexible filtering and embedding rather than nested routes.  One Thing Well  PostgREST has a focused scope. It works well with other tools like Nginx. This forces you to cleanly separate the data-centric CRUD operations from other concerns. Use a collection of sharp tools rather than building a big ball of mud.  Shared Improvements  As with any open source project, we all gain from features and fixes in the tool. It's more beneficial than improvements locked inextricably within custom codebases.  Myths  You have to make tons of stored procs and triggers  Modern PostgreSQL features like auto-updatable views and computed columns make this mostly unnecessary. Triggers do play a part, but generally not for irksome boilerplate. When they are required triggers are preferable to ad-hoc app code anyway, since the former work reliably for any codepath.  Exposing the database destroys encapsulation  PostgREST does versioning through database schemas. This allows you to expose tables and views without making the app brittle. Underlying tables can be superseded and hidden behind public facing views. The chapter about versioning shows how to do this.  Conventions  This guide contains highlighted notes and tangential information interspersed with the text.  \n     Design Consideration \n\n     Contains history which informed the current design. Sometimes it discusses unavoidable tradeoffs or a point of theory.   \n     Invitation to Contribute \n\n     Points out things we know we want to add or improve. They might give you ideas for ways to contribute to the project.   \n     Deprecation Warning \n\n     Alerts you to features which will be removed in the next major (breaking) release.", 
            "title": "Introduction"
        }, 
        {
            "location": "/install/server/", 
            "text": "Installation\n\n\nInstalling from Pre-Built Release\n\n\nThe \nrelease page\n has precompiled binaries for Mac OS X and 64-bit Ubuntu. Next extract the tarball and run the binary inside with no arguments to see usage instructions:\n\n\n# Untar the release (available at https://github.com/begriffs/postgrest/releases/latest)\n\n$ tar zxf postgrest-[version]-[platform].tar.xz\n\n# Try running it\n$ ./postgrest\n\n# You should see a usage help message\n\n\n\n\n\n    \nInvitation to Contribute\n\n\n    \nI currently build the binaries manually for each version. We need to set up an automated build matrix for various architectures. It should support 32- and 64-bit versions of\n\n    \nScientific Linux 6\nCentOS\nRHEL 6\n\n\n    Also it would be good to create a package for apt.\n\n\n\n\n\nWe'll learn the meaning of the command line flags later, but here is a minimal example of running the app. It does all operations as user \npostgres\n, including for unauthenticated requests.\n\n\n$ ./postgrest -d dbname -U postgres -a postgres --v1schema public\n\n\n\n\nBuilding from Source\n\n\nWhen a prebuilt binary does not exist for your system you can build the project from source. You'll also need to do this if you want to help with development. \nStack\n makes it easy. It will install any necessary Haskell dependencies on your system.\n\n\n\n\nInstall Stack\n for your platform\n\n\n\n\n#ubuntu example\nwget -q -O- https://s3.amazonaws.com/download.fpcomplete.com/ubuntu/fpco.key | sudo apt-key add -\necho 'deb http://download.fpcomplete.com/ubuntu/trusty stable main'|sudo tee /etc/apt/sources.list.d/fpco.list\nsudo apt-get update \n sudo apt-get install stack -y\n\n\n\n\n\n\nBuild \n install in one step\n\n\n\n\ngit clone https://github.com/begriffs/postgrest.git\ncd postgrest\nsudo stack install --install-ghc --local-bin-path /usr/local/bin\n\n\n\n\n\n\nRun the server\n\n\n\n\npostgrest dbconnectionstring arg1 arg2\n\n\n\n\nIf you want to run the test suite, stack can do that too: \nstack test\n.\n\n\nInstalling PostgreSQL\n\n\nTo use PostgREST you will need an underlying database. You can use something like Amazon \nRDS\n but installing your own locally is cheaper and more convenient for development.\n\n\n\n\nInstructions for OS X\n\n\nInstructions for Ubuntu 14.04", 
            "title": "The Server"
        }, 
        {
            "location": "/install/server/#installation", 
            "text": "Installing from Pre-Built Release  The  release page  has precompiled binaries for Mac OS X and 64-bit Ubuntu. Next extract the tarball and run the binary inside with no arguments to see usage instructions:  # Untar the release (available at https://github.com/begriffs/postgrest/releases/latest)\n\n$ tar zxf postgrest-[version]-[platform].tar.xz\n\n# Try running it\n$ ./postgrest\n\n# You should see a usage help message  \n     Invitation to Contribute \n\n     I currently build the binaries manually for each version. We need to set up an automated build matrix for various architectures. It should support 32- and 64-bit versions of\n\n     Scientific Linux 6 CentOS RHEL 6 \n\n    Also it would be good to create a package for apt.   We'll learn the meaning of the command line flags later, but here is a minimal example of running the app. It does all operations as user  postgres , including for unauthenticated requests.  $ ./postgrest -d dbname -U postgres -a postgres --v1schema public  Building from Source  When a prebuilt binary does not exist for your system you can build the project from source. You'll also need to do this if you want to help with development.  Stack  makes it easy. It will install any necessary Haskell dependencies on your system.   Install Stack  for your platform   #ubuntu example\nwget -q -O- https://s3.amazonaws.com/download.fpcomplete.com/ubuntu/fpco.key | sudo apt-key add -\necho 'deb http://download.fpcomplete.com/ubuntu/trusty stable main'|sudo tee /etc/apt/sources.list.d/fpco.list\nsudo apt-get update   sudo apt-get install stack -y   Build   install in one step   git clone https://github.com/begriffs/postgrest.git\ncd postgrest\nsudo stack install --install-ghc --local-bin-path /usr/local/bin   Run the server   postgrest dbconnectionstring arg1 arg2  If you want to run the test suite, stack can do that too:  stack test .  Installing PostgreSQL  To use PostgREST you will need an underlying database. You can use something like Amazon  RDS  but installing your own locally is cheaper and more convenient for development.   Instructions for OS X  Instructions for Ubuntu 14.04", 
            "title": "Installation"
        }, 
        {
            "location": "/install/ecosystem/", 
            "text": "Ecosystem\n\n\nClient-Side Libraries\n\n\n\n\nmithril.postgrest\n - Mithril plugin to create and authenticate requests\n\n\nlewisjared/postgrest-request\n - node interface to postgrest instances\n\n\nJarvusInnovations/jarvus-postgrest-apikit\n - Sencha framework package for binding models/stores/proxies to PostgREST tables\n\n\n\n\nExtensions\n\n\n\n\nsrid/spas\n - allow file uploads and basic auth\n\n\n\n\nExample Apps\n\n\n\n\nruslantalpa/blogdemo\n - blog api demo in a vagrant image\n\n\ntimwis/ext-postgrest-crud\n - browser-based spreadsheet\n\n\nsrid/chronicle\n - tracking a tree of personal memories\n\n\nbegriffs/postgrest-example\n - how to configure a db for use as an API\n\n\nmarmelab/ng-admin-postgrest\n - automatic database admin panel\n\n\ntyrchen/goodfilm\n - example film api\n\n\n\n\nIn Production\n\n\n\n\nCatarse", 
            "title": "Ecosystem"
        }, 
        {
            "location": "/install/ecosystem/#ecosystem", 
            "text": "Client-Side Libraries   mithril.postgrest  - Mithril plugin to create and authenticate requests  lewisjared/postgrest-request  - node interface to postgrest instances  JarvusInnovations/jarvus-postgrest-apikit  - Sencha framework package for binding models/stores/proxies to PostgREST tables   Extensions   srid/spas  - allow file uploads and basic auth   Example Apps   ruslantalpa/blogdemo  - blog api demo in a vagrant image  timwis/ext-postgrest-crud  - browser-based spreadsheet  srid/chronicle  - tracking a tree of personal memories  begriffs/postgrest-example  - how to configure a db for use as an API  marmelab/ng-admin-postgrest  - automatic database admin panel  tyrchen/goodfilm  - example film api   In Production   Catarse", 
            "title": "Ecosystem"
        }, 
        {
            "location": "/api/reading/", 
            "text": "Requesting Information\n\n\nTables and Views\n\n\n\n\n\u2705 Cacheable, prefetchable\n\n\n\u2705 Idempotent\n\n\n\n\nThe list of accessible tables and views is provided at\n\n\nGET /\n\n\n\n\nEvery view and table accessible by the active db role is exposed\nin a one-level deep route. For instance the full contents of a table\n\npeople\n is returned at\n\n\nGET /people\n\n\n\n\nThere are no \ndeeply/nested/routes\n. Each route provides \nOPTIONS\n,\n\nGET\n, \nPOST\n, \nPATCH\n, and \nDELETE\n verbs depending entirely\non database permissions.\n\n\n\n  \nDesign Consideration\n\n\n  \nWhy not provide nested routes? Many APIs allow nesting to\n  retrieve related information, such as \n/films/1/director\n.\n  We offer a more flexible mechanism (inspired by GraphQL) to embed\n  related information. It can handle one-to-many and many-to-many\n  relationships. This is covered in the section about Embedding.\n\n\n\n\n\nStored Procedures\n\n\n\n\n\u274c Cannot necessarily be cached or prefetched\n\n\n\u274c Not necessarily idempotent\n\n\n\n\nEvery stored procedure is accessible under the \n/rpc\n prefix. The\nAPI endpoint supports only POST which executes the function.\n\n\nPOST /rpc/proc_name\n\n\n\n\nPostgREST supports calling procedures with \nnamed\narguments\n.\nInclude a JSON object in the request payload and each\nkey/value of the object will become an argument.\n\n\n\n  \nDesign Consideration\n\n\n  \nWhy the /rpc prefix? One reason is to avoid name collisions\n  between views and procedures. It also helps emphasize to API\n  consumers that these functions are not normal restful things.\n  The functions can have arbitrary and surprising behavior, not\n  the standard \"post creates a resource\" thing that users expect\n  from the other routes.\n\n\n  \nWe considered allowing GET requests for functions that are\n  marked non-volatile but could not reconcile how to pass in\n  parameters. Query string arguments are reserved for shaping/filtering\n  the output, not providing input.\n\n\n\n\n\nFiltering\n\n\nFiltering Rows\n\n\nYou can filter result rows by adding conditions on columns, each\ncondition a query string parameter.  For instance, to return people\naged under 13 years old:\n\n\nGET /people?age=lt.13\n\n\n\n\nAdding multiple parameters conjoins the conditions:\n\n\nGET /people?age=gte.18\nstudent=is.true\n\n\n\n\nThese operators are available:\n\n\n\n\n\n\n\n\nabbreviation\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\neq\n\n\nequals\n\n\n\n\n\n\ngt\n\n\ngreater than\n\n\n\n\n\n\nlt\n\n\nless than\n\n\n\n\n\n\ngte\n\n\ngreater than or equal\n\n\n\n\n\n\nlte\n\n\nless than or equal\n\n\n\n\n\n\nlike\n\n\nLIKE operator (use * in place of %)\n\n\n\n\n\n\nilike\n\n\nILIKE operator (use * in place of %)\n\n\n\n\n\n\n@@\n\n\nfull-text search using to_tsquery\n\n\n\n\n\n\nis\n\n\nchecking for exact equality (null,true,false)\n\n\n\n\n\n\nin\n\n\none of a list of values e.g. \n?a=in.1,2,3\n\n\n\n\n\n\nnot\n\n\nnegates another operator, see below\n\n\n\n\n\n\n\n\nTo negate any operator, prefix it with \nnot\n like \n?a=not.eq.2\n.\n\n\nFor more complicated filters (such as those involving condition 1\n\nOR\n condition 2) you will have to create a new view in the database.\n\n\nFilters may be applied to \ncomputed\ncolumns\n\nas well as actual table/view columns, even though the computed\ncolumns will not appear in the output.\n\n\nFiltering Columns\n\n\nYou can customize which columns are returned by using the \nselect\n\nparameter:\n\n\nGET /people?select=age,height,weight\n\n\n\n\nTo cast the column types, add a double colon\n\n\nGET /people?select=age::text,height,weight\n\n\n\n\nNot all type coercions are possible, and you will get an error\ndescribing any problems from selection or type casting.\n\n\nThe \nselect\n keyword is reserved. You thus cannot filter rows based\non a column named select. Then again it is a reserved SQL keyword\ntoo, hence an unlikely column name.\n\n\nInside JSONB\n\n\nPostgreSQL \n=9.4.2 supports native JSON columns and can even index\nthem by internal keys using the \njsonb\n column type. PostgREST\nallows you to filter results by internal JSON object values. Use\nthe single- and double-arrows to path into and obtain values, e.g.\n\n\nGET /stuff?json_col-\na-\nb=eq.2\n\n\n\n\nThis query finds rows in \nstuff\n where \njson_col-\n'a'-\n'b'\n is\nequal to 2 (or \"2\" -- it coerces as needed). The final arrow must\nbe the double kind, \n-\n, or else PostgREST will not attempt to\nlook inside the JSON.\n\n\nOrdering\n\n\nThe reserved word \norder\n reorders the response rows.  It uses a\ncomma-separated list of columns and directions:\n\n\nGET /people?order=age.desc,height.asc\n\n\n\n\nIf no direction is specified it defaults to descending order:\n\n\nGET /people?order=age\n\n\n\n\nIf you care where nulls are sorted, add \nnullsfirst\n or \nnullslast\n:\n\n\nGET /people?order=age.nullsfirst\n\n\n\n\nLimiting and Pagination\n\n\nPagination by Limit-Offset\n\n\nPostgREST uses HTTP range headers for limiting and describing the\nsize of results. Every response contains the current range and total\nresults:\n\n\nRange-Unit: items\nContent-Range \u2192 0-14/15\n\n\n\n\nThis means items zero through fourteen are returned out of a total\nof fifteen -- i.e. all of them. This information is available in\nevery response and can help you render pagination controls on the\nclient. This is a RFC7233-compliant solution that keeps the response\nJSON cleaner.\n\n\nThe client can set the limit and offset of a request by setting the\n\nRange\n header. Translate the limit and offset into a range. To\nrequest the first five elements, include these request headers:\n\n\nRange-Unit: items\nRange: 0-4\n\n\n\n\nYou can also use open-ended ranges for an offset with no limit:\n\nRange: 10-\n.\n\n\nSuppressing Counts\n\n\nSometimes knowing the total row count of a query is unnecessary and\nonly adds extra cost to the database query. So you can skip the\ncount total using a \nPrefer\n header as:\n\n\nPrefer: count=none\n\n\n\n\nWith count suppressed the PostgREST response will look like:\n\n\nRange-Unit: items\nContent-Range \u2192 0-14/*\n\n\n\n\nEmbedding Foreign Entities\n\n\nSuppose you have a \nprojects\n table which references \nclients\n through\na foreign key called \nclient_id\n. When listing projects through the\nAPI you can have it embed the client within each project response.\nFor example,\n\n\nGET /projects?id=eq.1\nselect=id, name, clients{*}\n\n\n\n\nNotice this is the same \nselect\n keyword which is used to choose\nwhich columns to include. When a column name is followed by parentheses\nthat means to fetch the entire record and nest it. You include a\nlist of columns inside the parens, or asterisk to request all\ncolumns.\n\n\nThe embedding works for 1-N, N-1, and N-N relationships. That means\nyou could also ask for a client and all their projects:\n\n\nGET /clients?id=eq.42\nselect=id, name, projects{*}\n\n\n\n\nIn the examples above we asked for all columns in the embedded resource\nbut the the select query is recursive. You could for instance specify\n\n\nGET /foo?select=x, y, bar{z, w, baz{*}}\n\n\n\n\nResponse Format\n\n\nQuery responses default to JSON but you can get them in CSV as well. Just make your request with the header\n\n\nAccept: text/csv\n\n\n\n\nSingular vs Plural\n\n\nMany APIs distinguish plural and singular resources, e.g.\n/stories\n\nvs \n/stories/1\n. Why do we use \n/stories?id=eq.1\n? It is because a\nsingle resource is for us a row determined by a primary key, and\nprimary keys can be \ncompound\n (meaning defined across more than\none column). The common urls come from a degenerate case of simple\n(and overwhelmingly numeric) primary keys often introduced automatically\nbe Object Relational Mapping.\n\n\nFor consistency's sake all these endpoints return a JSON array,\n\n/stories\n, \n/stories?genre=eq.mystery\n, \n/stories?id=eq.1\n. They\nare all filtering a bigger array. However you might want the\nlast one to return a single JSON object, not an array with one\nelement. To request a singular response send the header\n\nPrefer: plurality=singular\n.\n\n\nData Schema\n\n\nAs well as issuing a \nGET /\n to obtain a list of the tables, views,\nand stored procedures available, you can get more information about\nany particular endpoint.\n\n\nOPTIONS /my_view\n\n\n\n\nThis will include the row names, their types, primary key\ninformation, and foreign keys for the given table or view.\n\n\n\n    \nDeprecation Warning\n\n\n    \nAlthough we currently use the OPTIONS verb for this, some\n    people \nargue\n that\n    this is inappropriate. We are considering a \ndescribedby\n\n    header link instead.\n\n\n\n\n\nCORS\n\n\nPostgREST sets highly permissive cross origin resource sharing.  It\naccepts Ajax requests from any domain.", 
            "title": "Reading"
        }, 
        {
            "location": "/api/reading/#requesting-information", 
            "text": "Tables and Views   \u2705 Cacheable, prefetchable  \u2705 Idempotent   The list of accessible tables and views is provided at  GET /  Every view and table accessible by the active db role is exposed\nin a one-level deep route. For instance the full contents of a table people  is returned at  GET /people  There are no  deeply/nested/routes . Each route provides  OPTIONS , GET ,  POST ,  PATCH , and  DELETE  verbs depending entirely\non database permissions.  \n   Design Consideration \n\n   Why not provide nested routes? Many APIs allow nesting to\n  retrieve related information, such as  /films/1/director .\n  We offer a more flexible mechanism (inspired by GraphQL) to embed\n  related information. It can handle one-to-many and many-to-many\n  relationships. This is covered in the section about Embedding.   Stored Procedures   \u274c Cannot necessarily be cached or prefetched  \u274c Not necessarily idempotent   Every stored procedure is accessible under the  /rpc  prefix. The\nAPI endpoint supports only POST which executes the function.  POST /rpc/proc_name  PostgREST supports calling procedures with  named\narguments .\nInclude a JSON object in the request payload and each\nkey/value of the object will become an argument.  \n   Design Consideration \n\n   Why the /rpc prefix? One reason is to avoid name collisions\n  between views and procedures. It also helps emphasize to API\n  consumers that these functions are not normal restful things.\n  The functions can have arbitrary and surprising behavior, not\n  the standard \"post creates a resource\" thing that users expect\n  from the other routes. \n\n   We considered allowing GET requests for functions that are\n  marked non-volatile but could not reconcile how to pass in\n  parameters. Query string arguments are reserved for shaping/filtering\n  the output, not providing input.   Filtering  Filtering Rows  You can filter result rows by adding conditions on columns, each\ncondition a query string parameter.  For instance, to return people\naged under 13 years old:  GET /people?age=lt.13  Adding multiple parameters conjoins the conditions:  GET /people?age=gte.18 student=is.true  These operators are available:     abbreviation  meaning      eq  equals    gt  greater than    lt  less than    gte  greater than or equal    lte  less than or equal    like  LIKE operator (use * in place of %)    ilike  ILIKE operator (use * in place of %)    @@  full-text search using to_tsquery    is  checking for exact equality (null,true,false)    in  one of a list of values e.g.  ?a=in.1,2,3    not  negates another operator, see below     To negate any operator, prefix it with  not  like  ?a=not.eq.2 .  For more complicated filters (such as those involving condition 1 OR  condition 2) you will have to create a new view in the database.  Filters may be applied to  computed\ncolumns \nas well as actual table/view columns, even though the computed\ncolumns will not appear in the output.  Filtering Columns  You can customize which columns are returned by using the  select \nparameter:  GET /people?select=age,height,weight  To cast the column types, add a double colon  GET /people?select=age::text,height,weight  Not all type coercions are possible, and you will get an error\ndescribing any problems from selection or type casting.  The  select  keyword is reserved. You thus cannot filter rows based\non a column named select. Then again it is a reserved SQL keyword\ntoo, hence an unlikely column name.  Inside JSONB  PostgreSQL  =9.4.2 supports native JSON columns and can even index\nthem by internal keys using the  jsonb  column type. PostgREST\nallows you to filter results by internal JSON object values. Use\nthe single- and double-arrows to path into and obtain values, e.g.  GET /stuff?json_col- a- b=eq.2  This query finds rows in  stuff  where  json_col- 'a'- 'b'  is\nequal to 2 (or \"2\" -- it coerces as needed). The final arrow must\nbe the double kind,  - , or else PostgREST will not attempt to\nlook inside the JSON.  Ordering  The reserved word  order  reorders the response rows.  It uses a\ncomma-separated list of columns and directions:  GET /people?order=age.desc,height.asc  If no direction is specified it defaults to descending order:  GET /people?order=age  If you care where nulls are sorted, add  nullsfirst  or  nullslast :  GET /people?order=age.nullsfirst  Limiting and Pagination  Pagination by Limit-Offset  PostgREST uses HTTP range headers for limiting and describing the\nsize of results. Every response contains the current range and total\nresults:  Range-Unit: items\nContent-Range \u2192 0-14/15  This means items zero through fourteen are returned out of a total\nof fifteen -- i.e. all of them. This information is available in\nevery response and can help you render pagination controls on the\nclient. This is a RFC7233-compliant solution that keeps the response\nJSON cleaner.  The client can set the limit and offset of a request by setting the Range  header. Translate the limit and offset into a range. To\nrequest the first five elements, include these request headers:  Range-Unit: items\nRange: 0-4  You can also use open-ended ranges for an offset with no limit: Range: 10- .  Suppressing Counts  Sometimes knowing the total row count of a query is unnecessary and\nonly adds extra cost to the database query. So you can skip the\ncount total using a  Prefer  header as:  Prefer: count=none  With count suppressed the PostgREST response will look like:  Range-Unit: items\nContent-Range \u2192 0-14/*  Embedding Foreign Entities  Suppose you have a  projects  table which references  clients  through\na foreign key called  client_id . When listing projects through the\nAPI you can have it embed the client within each project response.\nFor example,  GET /projects?id=eq.1 select=id, name, clients{*}  Notice this is the same  select  keyword which is used to choose\nwhich columns to include. When a column name is followed by parentheses\nthat means to fetch the entire record and nest it. You include a\nlist of columns inside the parens, or asterisk to request all\ncolumns.  The embedding works for 1-N, N-1, and N-N relationships. That means\nyou could also ask for a client and all their projects:  GET /clients?id=eq.42 select=id, name, projects{*}  In the examples above we asked for all columns in the embedded resource\nbut the the select query is recursive. You could for instance specify  GET /foo?select=x, y, bar{z, w, baz{*}}  Response Format  Query responses default to JSON but you can get them in CSV as well. Just make your request with the header  Accept: text/csv  Singular vs Plural  Many APIs distinguish plural and singular resources, e.g. /stories \nvs  /stories/1 . Why do we use  /stories?id=eq.1 ? It is because a\nsingle resource is for us a row determined by a primary key, and\nprimary keys can be  compound  (meaning defined across more than\none column). The common urls come from a degenerate case of simple\n(and overwhelmingly numeric) primary keys often introduced automatically\nbe Object Relational Mapping.  For consistency's sake all these endpoints return a JSON array, /stories ,  /stories?genre=eq.mystery ,  /stories?id=eq.1 . They\nare all filtering a bigger array. However you might want the\nlast one to return a single JSON object, not an array with one\nelement. To request a singular response send the header Prefer: plurality=singular .  Data Schema  As well as issuing a  GET /  to obtain a list of the tables, views,\nand stored procedures available, you can get more information about\nany particular endpoint.  OPTIONS /my_view  This will include the row names, their types, primary key\ninformation, and foreign keys for the given table or view.  \n     Deprecation Warning \n\n     Although we currently use the OPTIONS verb for this, some\n    people  argue  that\n    this is inappropriate. We are considering a  describedby \n    header link instead.   CORS  PostgREST sets highly permissive cross origin resource sharing.  It\naccepts Ajax requests from any domain.", 
            "title": "Requesting Information"
        }, 
        {
            "location": "/api/writing/", 
            "text": "Updating Data\n\n\nRecord Creation\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nTo create a row in a database table post a JSON object whose keys\nare the names of the columns you would like to create. Missing keys\nwill be set to default values when applicable.\n\n\nPOST /table_name\n{ \ncol1\n: \nvalue1\n, \ncol2\n: \nvalue2\n }\n\n\n\n\nThe response will include a \nLocation\n header describing where to\nfind the new object. If you would like to get the full object back\nin the response to your request, include the header \nPrefer:\nreturn=representation\n. That way you won't have to make another\nHTTP call to discover properties that may have been filled in on\nthe server side.\n\n\nBulk Insertion\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nYou can POST a JSON array or CSV to insert multiple rows in a single\nHTTP request. Note that using CSV requires less parsing on the server\nand is \nmuch faster\n.\n\n\nExample of CSV bulk insert. Simply post to a table route with\n\nContent-Type: text/csv\n and include the names of the columns as\nthe first row. For instance\n\n\nPOST /people\nname,age,height\nJ Doe,62,70\nJonas,10,55\n\n\n\n\nAn empty field (\n,,\n) is coerced to an empty string and the reserved\nword \nNULL\n is mapped to the SQL null value. Note that there should\nbe no spaces between the column names and commas.\n\n\nExample of JSON bulk insert. Send an array:\n\n\nPOST /people\n[\n  { \nname\n: \nJ Doe\n, \nage\n: 62, \nheight\n: 70 },\n  { \nname\n: \nJanus\n, \nage\n: 10, \nheight\n: 55 }\n]\n\n\n\n\nIf you would like to get the full object back in the response to\nyour request, include the header \nPrefer: return=representation\n.\nChances are you only want certain information back, though, like\ncreated ids. You can pass a \nselect\n parameter to affect the shape\nof the response (further documented in the \nreading\n\npage). For instance\n\n\nPOST /people?select=id\n[...]\n\n\n\n\nreturns something like\n\n\n[ { \nid\n: 1 }, { \nid\n: 2 } ]\n\n\n\n\nBulk Updates\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nTo change parts of a resource or resources use the \nPATCH\n verb.\nFor instance, here is how to mark all young people as children.\n\n\nPATCH /people?age=lt.13\n{\n  \nperson_type\n: \nchild\n\n}\n\n\n\n\nThis affects any rows matched by the url param filters, overwrites\nany fields specified in in the payload JSON and leaves the other\nfields unaffected. Note that although the payload is not in the\nJSON patch format specified by\n\nRFC6902\n, HTTP does not specify\nwhich patch format to use. Our format is more pleasant, meant for\nbasic field replacements, and not at all \"incorrect.\"\n\n\nDeletion\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u2705 Idempotent\n\n\n\n\nSimply use the \nDELETE\n verb. All recors that match your filter\nwill be removed. For instance deleting inactive users:\n\n\nDELETE /user?active=is.false\n\n\n\n\nProtecting Dangerous Actions\n\n\nNotice that it is very easy to delete or update many records at\nonce. In fact forgetting a filter will affect an entire table!\n\n\n\n    \nInvitation to Contribute\n\n\n    \nWe would like to investigate nginx rules to guard dangerous\n    actions, perhaps requiring a confirmation header or query param\n    to perform the action.\n\n\n    \nYou're invited to research this option and contribute to\n    this documentation.", 
            "title": "Writing"
        }, 
        {
            "location": "/api/writing/#updating-data", 
            "text": "Record Creation   \u274c Cannot be cached or prefetched  \u274c Not idempotent   To create a row in a database table post a JSON object whose keys\nare the names of the columns you would like to create. Missing keys\nwill be set to default values when applicable.  POST /table_name\n{  col1 :  value1 ,  col2 :  value2  }  The response will include a  Location  header describing where to\nfind the new object. If you would like to get the full object back\nin the response to your request, include the header  Prefer:\nreturn=representation . That way you won't have to make another\nHTTP call to discover properties that may have been filled in on\nthe server side.  Bulk Insertion   \u274c Cannot be cached or prefetched  \u274c Not idempotent   You can POST a JSON array or CSV to insert multiple rows in a single\nHTTP request. Note that using CSV requires less parsing on the server\nand is  much faster .  Example of CSV bulk insert. Simply post to a table route with Content-Type: text/csv  and include the names of the columns as\nthe first row. For instance  POST /people\nname,age,height\nJ Doe,62,70\nJonas,10,55  An empty field ( ,, ) is coerced to an empty string and the reserved\nword  NULL  is mapped to the SQL null value. Note that there should\nbe no spaces between the column names and commas.  Example of JSON bulk insert. Send an array:  POST /people\n[\n  {  name :  J Doe ,  age : 62,  height : 70 },\n  {  name :  Janus ,  age : 10,  height : 55 }\n]  If you would like to get the full object back in the response to\nyour request, include the header  Prefer: return=representation .\nChances are you only want certain information back, though, like\ncreated ids. You can pass a  select  parameter to affect the shape\nof the response (further documented in the  reading \npage). For instance  POST /people?select=id\n[...]  returns something like  [ {  id : 1 }, {  id : 2 } ]  Bulk Updates   \u274c Cannot be cached or prefetched  \u274c Not idempotent   To change parts of a resource or resources use the  PATCH  verb.\nFor instance, here is how to mark all young people as children.  PATCH /people?age=lt.13\n{\n   person_type :  child \n}  This affects any rows matched by the url param filters, overwrites\nany fields specified in in the payload JSON and leaves the other\nfields unaffected. Note that although the payload is not in the\nJSON patch format specified by RFC6902 , HTTP does not specify\nwhich patch format to use. Our format is more pleasant, meant for\nbasic field replacements, and not at all \"incorrect.\"  Deletion   \u274c Cannot be cached or prefetched  \u2705 Idempotent   Simply use the  DELETE  verb. All recors that match your filter\nwill be removed. For instance deleting inactive users:  DELETE /user?active=is.false  Protecting Dangerous Actions  Notice that it is very easy to delete or update many records at\nonce. In fact forgetting a filter will affect an entire table!  \n     Invitation to Contribute \n\n     We would like to investigate nginx rules to guard dangerous\n    actions, perhaps requiring a confirmation header or query param\n    to perform the action. \n\n     You're invited to research this option and contribute to\n    this documentation.", 
            "title": "Updating Data"
        }, 
        {
            "location": "/admin/security/", 
            "text": "Security\n\n\nSSL\n\n\nDatabase Roles\n\n\nJSON Web Tokens\n\n\nIssuing via sql procedures\n\n\nRow-Level Security\n\n\nSimulated - PostgreSQL \n9.5\n\n\nReal - PostgreSQL \n=9.5\n\n\nBuilding Auth on top of JWT\n\n\nBasic Auth\n\n\nGithub Sign-in", 
            "title": "Security"
        }, 
        {
            "location": "/admin/security/#security", 
            "text": "SSL  Database Roles  JSON Web Tokens  Issuing via sql procedures  Row-Level Security  Simulated - PostgreSQL  9.5  Real - PostgreSQL  =9.5  Building Auth on top of JWT  Basic Auth  Github Sign-in", 
            "title": "Security"
        }, 
        {
            "location": "/admin/versioning/", 
            "text": "API Versioning\n\n\nSchema Search Path\n\n\nChanging a Resource\n\n\nRemoving a Resource\n\n\nAvoiding DB and Client Coupling", 
            "title": "Versioning"
        }, 
        {
            "location": "/admin/versioning/#api-versioning", 
            "text": "Schema Search Path  Changing a Resource  Removing a Resource  Avoiding DB and Client Coupling", 
            "title": "API Versioning"
        }, 
        {
            "location": "/admin/migration/", 
            "text": "Data Migration\n\n\nSqitch\n\n\nTest-Driven Migrations\n\n\nStructural Tests\n\n\nValue Tests with pgTAP", 
            "title": "Migration"
        }, 
        {
            "location": "/admin/migration/#data-migration", 
            "text": "Sqitch  Test-Driven Migrations  Structural Tests  Value Tests with pgTAP", 
            "title": "Data Migration"
        }, 
        {
            "location": "/admin/deployment/", 
            "text": "Deployment\n\n\nHeroku\n\n\nGetting Started\n\n\nUsing Amazon RDS\n\n\nDebian", 
            "title": "Deployment"
        }, 
        {
            "location": "/admin/deployment/#deployment", 
            "text": "Heroku  Getting Started  Using Amazon RDS  Debian", 
            "title": "Deployment"
        }, 
        {
            "location": "/admin/performance/", 
            "text": "Performance\n\n\nBenchmarks\n\n\nCaching\n\n\nQuality of Service\n\n\nTips", 
            "title": "Performance"
        }, 
        {
            "location": "/admin/performance/#performance", 
            "text": "Benchmarks  Caching  Quality of Service  Tips", 
            "title": "Performance"
        }, 
        {
            "location": "/examples/start/", 
            "text": "Getting Started\n\n\nYour First (simple) API\n\n\nLet's start with the simplest thing possible. We will expose some tables directly for reading and writing by anyone.\n\n\nStart by making a database\n\n\ncreatedb demo1\n\n\n\n\nWe'll set it up with a film example (courtesy of \nJonathan Harrington\n). Copy the following into your clipboard:\n\n\nBEGIN;\n\nCREATE TABLE director\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE film\n(\n  id serial PRIMARY KEY,\n  title text NOT NULL,\n  year date NOT NULL,\n  director text REFERENCES director (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  rating real NOT NULL DEFAULT 0,\n  language text NOT NULL\n);\n\nCREATE TABLE festival\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE competition\n(\n  id serial PRIMARY KEY,\n  name text NOT NULL,\n  festival text NOT NULL REFERENCES festival (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  year date NOT NULL\n);\n\nCREATE TABLE film_nomination\n(\n  id serial PRIMARY KEY,\n  competition integer NOT NULL REFERENCES competition (id)\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  film integer NOT NULL REFERENCES film (id)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  won boolean NOT NULL DEFAULT true\n);\n\nCOMMIT;\n\n\n\n\nApply it to your new database by running\n\n\n# On OS X\npbpaste | psql demo1\n\n# Or Linux\n# xclip -selection clipboard -o | psql demo1\n\n\n\n\nStart the PostgREST server and point it at the new database. (See the \ninstallation instructions\n.)\n\n\npostgrest postgres://postgres:@localhost:5432/demo1 -a postgres --schema public\n\n\n\n\n\n    \nNote about database users\n\n\n    \nIf you installed PostgreSQL with Homebrew on Mac then the\n    database username may be your own login rather than\n    \npostgres\n.\n\n\n\n\n\nPopulating Data\n\n\nLet's use PostgREST to populate the database. Install a REST client such as \nPostman\n. Now let's insert some data as a bulk post in CSV format:\n\n\nPOST http://localhost:3000/festival\nContent-Type: text/csv\n\nname\nVenice Film Festival\nCannes Film Festival\n\n\n\n\nIn Postman it will look like this\n\n\n\n\nNotice that the post type is \nraw\n and that \nContent-Type: text/csv\n set in the Headers tab.\n\n\nThe server returns HTTP 201 Created. Because we inserted more than one item at once there is no \nLocation\n header in the response. However sometimes you want to learn more about items which you just inserted. To have the server include the full restuls include the header \nPrefer: return=representation\n.\n\n\nAt this point if you send a GET request to \n/festival\n it should return\n\n\n[\n  {\n    \nname\n: \nVenice Film Festival\n\n  },\n  {\n    \nname\n: \nCannes Film Festival\n\n  }\n]\n\n\n\n\nNow that you've seen how to do a bulk insert, let's do some more and fully populate the database.\n\n\nPost the following to \n/competition\n:\n\n\nname,festival,year\nGolden Lion,Venice Film Festival,2014-01-01\nPalme d'Or,Cannes Film Festival,2014-01-01\n\n\n\n\nNow \n/director\n:\n\n\nname\nBertrand Bonello\nAtom Egoyan\nDavid Gordon Green\nAndrey Konchalovskiy\nMario Martone\nMike Leigh\nRoy Andersson\nSaverio Costanzo\nAlix Delaporte\nJean-Pierre Dardenne\nXiaoshuai Wang\nKaan M\u00fcjdeci\nTommy Lee Jones\nNuri Bilge Ceylan\nMichel Hazanavicius\nXavier Dolan\nRamin Bahrani\nAlice Rohrwacher\nAndrew Niccol\nRakhshan Bani-Etemad\nDavid Oelhoffen\nBennett Miller\nDavid Cronenberg\nShin'ya Tsukamoto\nJoshua Oppenheimer\nOlivier Assayas\nJean-Luc Godard\nAlejandro Gonz\u00e1lez I\u00f1\u00e1rritu\nBeno\u00eet Jacquot\nFatih Akin\nFrancesco Munzi\nKen Loach\nAbel Ferrara\nXavier Beauvois\nNaomi Kawase\n\n\n\n\nAnd \n/film\n:\n\n\ntitle,year,director,rating,language\nChuang ru zhe,2014-01-01,Xiaoshuai Wang,6.19999981,english\nThe Look of Silence,2014-01-01,Joshua Oppenheimer,8.30000019,Indonesian\nFires on the Plain,2014-01-01,Shin'ya Tsukamoto,5.80000019,Japanese\nFar from Men,2014-01-01,David Oelhoffen,7.5,english\nGood Kill,2014-01-01,Andrew Niccol,6.0999999,english\nLeopardi,2014-01-01,Mario Martone,6.9000001,english\nSivas,2014-01-01,Kaan M\u00fcjdeci,7.69999981,english\nBlack Souls,2014-01-01,Francesco Munzi,7.0999999,english\nThree Hearts,2014-01-01,Beno\u00eet Jacquot,5.80000019,French\nPasolini,2014-01-01,Abel Ferrara,5.80000019,english\nLe dernier coup de marteau,2014-01-01,Alix Delaporte,6.5,english\nManglehorn,2014-01-01,David Gordon Green,7.0999999,english\nHungry Hearts,2014-01-01,Saverio Costanzo,6.4000001,English\nBelye nochi pochtalona Alekseya Tryapitsyna,2014-01-01,Andrey Konchalovskiy,6.9000001,Russian\n99 Homes,2014-01-01,Ramin Bahrani,7.30000019,english\nThe Cut,2014-01-01,Fatih Akin,6,Armenian\nBirdman: Or (The Unexpected Virtue of Ignorance),2014-01-01,Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu,8,English\nLa ran\u00e7on de la gloire,2014-01-01,Xavier Beauvois,5.69999981,French\nA Pigeon Sat on a Branch Reflecting on Existence,2014-01-01,Roy Andersson,7.19999981,english\nTales,2014-01-01,Rakhshan Bani-Etemad,6.80000019,english\nThe Wonders,2014-01-01,Alice Rohrwacher,6.80000019,Italian\nFoxcatcher,2014-01-01,Bennett Miller,7.19999981,English\nMr. Turner,2014-01-01,Mike Leigh,7,English\nJimmy's Hall,2014-01-01,Ken Loach,6.69999981,English\nThe Homesman,2014-01-01,Tommy Lee Jones,6.5999999,English\nThe Captive,2014-01-01,Atom Egoyan,5.9000001,english\nGoodbye to Language,2014-01-01,Jean-Luc Godard,6.19999981,French\nThe Search,2014-01-01,Michel Hazanavicius,6.9000001,French\nStill the Water,2014-01-01,Naomi Kawase,6.9000001,Japanese\nMommy,2014-01-01,Xavier Dolan,8.30000019,French\n\nTwo Days, One Night\n,2014-01-01,Jean-Pierre Dardenne,7.4000001,French\nMaps to the Stars,2014-01-01,David Cronenberg,6.4000001,English\nSaint Laurent,2014-01-01,Bertrand Bonello,6.5,French\nClouds of Sils Maria,2014-01-01,Olivier Assayas,6.9000001,english\nWinter Sleep,2014-01-01,Nuri Bilge Ceylan,8.5,Turkish\n\n\n\n\nFinally \n/film_nomination\n:\n\n\ncompetition,film,won\n1,1,f\n1,2,f\n1,3,f\n1,4,f\n1,5,f\n1,6,f\n1,7,f\n1,8,f\n1,9,f\n1,10,f\n1,11,f\n1,12,f\n1,13,f\n1,14,f\n1,15,f\n1,16,f\n1,17,f\n1,18,f\n1,19,f\n1,20,f\n2,21,f\n2,22,f\n2,23,f\n2,24,f\n2,25,f\n2,26,f\n2,27,f\n2,28,f\n2,29,f\n2,30,f\n2,31,f\n2,32,f\n2,33,f\n2,34,f\n2,35,f\n\n\n\n\nGetting and Embedding Data\n\n\nFirst let's review which films are stored in the database:\n\n\nGET http://localhost:3000/film\n\n\n\n\nIt gives us back a list of JSON objects. What if we care only about the film titles? Use \nselect\n to shape the output:\n\n\nGET http://localhost:3000/film?select=title\n\n\n\n\n[\n  {\n    \ntitle\n: \nChuang ru zhe\n\n  },\n  {\n    \ntitle\n: \nThe Look of Silence\n\n  },\n  {\n    \ntitle\n: \nFires on the Plain\n\n  },\n  ...\n]\n\n\n\n\nHere is where it gets cool. PostgREST can embed objects in its response through foreign key relationships. Earlier we created a join table called \nfilm_nomination\n. It joins films and competitions. We can ask the server about the structure of this table:\n\n\nOPTIONS http://localhost:3000/film_nomination\n\n\n\n\n{\n  \npkey\n: [\n    \nid\n\n  ],\n  \ncolumns\n: [\n    {\n      \nreferences\n: null,\n      \ndefault\n: \nnextval('film_nomination_id_seq'::regclass)\n,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nid\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 1\n    },\n    {\n      \nreferences\n: {\n        \nschema\n: \npublic\n,\n        \ncolumn\n: \nid\n,\n        \ntable\n: \ncompetition\n\n      },\n      \ndefault\n: null,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \ncompetition\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 2\n    },\n    {\n      \nreferences\n: {\n        \nschema\n: \npublic\n,\n        \ncolumn\n: \nid\n,\n        \ntable\n: \nfilm\n\n      },\n      \ndefault\n: null,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nfilm\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 3\n    },\n    {\n      \nreferences\n: null,\n      \ndefault\n: \ntrue\n,\n      \nprecision\n: null,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nwon\n,\n      \ntype\n: \nboolean\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 4\n    }\n  ]\n}\n\n\n\n\nFrom this you can see that the columns \nfilm\n and \ncompetition\n reference their eponymous tables. Let's ask the server for each film along with names of the competitions it entered. You don't have to do any custom coding. Send this query:\n\n\nGET http://localhost:3000/film?select=title,competition{name}\n\n\n\n\n[\n  {\n    \ntitle\n: \nChuang ru zhe\n,\n    \ncompetition\n: [\n      {\n        \nname\n: \nGolden Lion\n\n      }\n    ]\n  },\n  {\n    \ntitle\n: \nThe Look of Silence\n,\n    \ncompetition\n: [\n      {\n        \nname\n: \nGolden Lion\n\n      }\n    ]\n  },\n  ...\n]\n\n\n\n\nThe relation flows both ways. Here is how to get the name of each competition's name and the movies shown at it.\n\n\nGET http://localhost:3000/competition?select=name,film{title}\n\n\n\n\n[\n  {\n    \nname\n: \nGolden Lion\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nChuang ru zhe\n\n      },\n      {\n        \ntitle\n: \nThe Look of Silence\n\n      },\n      ...\n    ]\n  },\n  {\n    \nname\n: \nPalme d'Or\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nThe Wonders\n\n      },\n      {\n        \ntitle\n: \nFoxcatcher\n\n      },\n      ...\n    ]\n  }\n]\n\n\n\n\nWhy not learn about the directors too? There is a many-to-one relation directly between films and directors. We can alter our previous query to include directors in its results.\n\n\nGET http://localhost:3000/competition?select=name,film{title,director{*}}\n\n\n\n\n[\n  {\n    \nname\n: \nGolden Lion\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nManglehorn\n,\n        \ndirector\n: {\n          \nname\n: \nDavid Gordon Green\n\n        }\n      },\n      {\n        \ntitle\n: \nBelye nochi pochtalona Alekseya Tryapitsyna\n,\n        \ndirector\n: {\n          \nname\n: \nAndrey Konchalovskiy\n\n        }\n      },\n      ...\n    ]\n  },\n  ...\n]\n\n\n\n\nSingular Responses\n\n\nHow do we ask for a single film, for instance the second one we inserted?\n\n\nGET http://localhost:3000/film?id=eq.2\n\n\n\n\nIt returns\n\n\n[\n  {\n    \nid\n: 2,\n    \ntitle\n: \nThe Look of Silence\n,\n    \nyear\n: \n2014-01-01\n,\n    \ndirector\n: \nJoshua Oppenheimer\n,\n    \nrating\n: 8.3,\n    \nlanguage\n: \nIndonesian\n\n  }\n]\n\n\n\n\nLike any query, it gives us a result \nset\n, in this case an array with one element. However you and I know that \nid\n is a primary key, it will never return more than one result. We might want it returned as a JSON object, not an array. To express this preference include the header \nPrefer: plurality=singular\n. It will respond with\n\n\n{\n  \nid\n: 2,\n  \ntitle\n: \nThe Look of Silence\n,\n  \nyear\n: \n2014-01-01\n,\n  \ndirector\n: \nJoshua Oppenheimer\n,\n  \nrating\n: 8.3,\n  \nlanguage\n: \nIndonesian\n\n}\n\n\n\n\n\n    \nWhy this approach to singular responses?\n\n\n    \n\n    PostgREST knows which columns comprise a primary key for a\n    table, so why not automatically choose plurality=singular when\n    these column filters are present? The fact is it could come as a\n    shock to a client that by adding one more filter condition it can\n    change the entire response format.\n    \n\n    \n\n    Then why not expose another kind of route such as /film/2 to indicate\n    one particular film? Because this does not accommodate compound keys.\n    The convention complects a plurality preference with table key\n    assumptions. We should separate concerns.\n    \n\n    \n\n    It turns out you can still have routes like /film/2.  Use a\n    proxy such as Nginx. It can rewrite routes such as /films/2\n    into /films?id=eq.2 and add the Prefer header to make the results\n    singular.\n    \n\n\n\n\n\nConclusion\n\n\nThis tutorial showed how to create a database with a basic schema, run PostgREST, and interact with the API. The next tutorial will show how to enable security for a multi-tenant blogging API.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/examples/start/#getting-started", 
            "text": "Your First (simple) API  Let's start with the simplest thing possible. We will expose some tables directly for reading and writing by anyone.  Start by making a database  createdb demo1  We'll set it up with a film example (courtesy of  Jonathan Harrington ). Copy the following into your clipboard:  BEGIN;\n\nCREATE TABLE director\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE film\n(\n  id serial PRIMARY KEY,\n  title text NOT NULL,\n  year date NOT NULL,\n  director text REFERENCES director (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  rating real NOT NULL DEFAULT 0,\n  language text NOT NULL\n);\n\nCREATE TABLE festival\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE competition\n(\n  id serial PRIMARY KEY,\n  name text NOT NULL,\n  festival text NOT NULL REFERENCES festival (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  year date NOT NULL\n);\n\nCREATE TABLE film_nomination\n(\n  id serial PRIMARY KEY,\n  competition integer NOT NULL REFERENCES competition (id)\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  film integer NOT NULL REFERENCES film (id)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  won boolean NOT NULL DEFAULT true\n);\n\nCOMMIT;  Apply it to your new database by running  # On OS X\npbpaste | psql demo1\n\n# Or Linux\n# xclip -selection clipboard -o | psql demo1  Start the PostgREST server and point it at the new database. (See the  installation instructions .)  postgrest postgres://postgres:@localhost:5432/demo1 -a postgres --schema public  \n     Note about database users \n\n     If you installed PostgreSQL with Homebrew on Mac then the\n    database username may be your own login rather than\n     postgres .   Populating Data  Let's use PostgREST to populate the database. Install a REST client such as  Postman . Now let's insert some data as a bulk post in CSV format:  POST http://localhost:3000/festival\nContent-Type: text/csv\n\nname\nVenice Film Festival\nCannes Film Festival  In Postman it will look like this   Notice that the post type is  raw  and that  Content-Type: text/csv  set in the Headers tab.  The server returns HTTP 201 Created. Because we inserted more than one item at once there is no  Location  header in the response. However sometimes you want to learn more about items which you just inserted. To have the server include the full restuls include the header  Prefer: return=representation .  At this point if you send a GET request to  /festival  it should return  [\n  {\n     name :  Venice Film Festival \n  },\n  {\n     name :  Cannes Film Festival \n  }\n]  Now that you've seen how to do a bulk insert, let's do some more and fully populate the database.  Post the following to  /competition :  name,festival,year\nGolden Lion,Venice Film Festival,2014-01-01\nPalme d'Or,Cannes Film Festival,2014-01-01  Now  /director :  name\nBertrand Bonello\nAtom Egoyan\nDavid Gordon Green\nAndrey Konchalovskiy\nMario Martone\nMike Leigh\nRoy Andersson\nSaverio Costanzo\nAlix Delaporte\nJean-Pierre Dardenne\nXiaoshuai Wang\nKaan M\u00fcjdeci\nTommy Lee Jones\nNuri Bilge Ceylan\nMichel Hazanavicius\nXavier Dolan\nRamin Bahrani\nAlice Rohrwacher\nAndrew Niccol\nRakhshan Bani-Etemad\nDavid Oelhoffen\nBennett Miller\nDavid Cronenberg\nShin'ya Tsukamoto\nJoshua Oppenheimer\nOlivier Assayas\nJean-Luc Godard\nAlejandro Gonz\u00e1lez I\u00f1\u00e1rritu\nBeno\u00eet Jacquot\nFatih Akin\nFrancesco Munzi\nKen Loach\nAbel Ferrara\nXavier Beauvois\nNaomi Kawase  And  /film :  title,year,director,rating,language\nChuang ru zhe,2014-01-01,Xiaoshuai Wang,6.19999981,english\nThe Look of Silence,2014-01-01,Joshua Oppenheimer,8.30000019,Indonesian\nFires on the Plain,2014-01-01,Shin'ya Tsukamoto,5.80000019,Japanese\nFar from Men,2014-01-01,David Oelhoffen,7.5,english\nGood Kill,2014-01-01,Andrew Niccol,6.0999999,english\nLeopardi,2014-01-01,Mario Martone,6.9000001,english\nSivas,2014-01-01,Kaan M\u00fcjdeci,7.69999981,english\nBlack Souls,2014-01-01,Francesco Munzi,7.0999999,english\nThree Hearts,2014-01-01,Beno\u00eet Jacquot,5.80000019,French\nPasolini,2014-01-01,Abel Ferrara,5.80000019,english\nLe dernier coup de marteau,2014-01-01,Alix Delaporte,6.5,english\nManglehorn,2014-01-01,David Gordon Green,7.0999999,english\nHungry Hearts,2014-01-01,Saverio Costanzo,6.4000001,English\nBelye nochi pochtalona Alekseya Tryapitsyna,2014-01-01,Andrey Konchalovskiy,6.9000001,Russian\n99 Homes,2014-01-01,Ramin Bahrani,7.30000019,english\nThe Cut,2014-01-01,Fatih Akin,6,Armenian\nBirdman: Or (The Unexpected Virtue of Ignorance),2014-01-01,Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu,8,English\nLa ran\u00e7on de la gloire,2014-01-01,Xavier Beauvois,5.69999981,French\nA Pigeon Sat on a Branch Reflecting on Existence,2014-01-01,Roy Andersson,7.19999981,english\nTales,2014-01-01,Rakhshan Bani-Etemad,6.80000019,english\nThe Wonders,2014-01-01,Alice Rohrwacher,6.80000019,Italian\nFoxcatcher,2014-01-01,Bennett Miller,7.19999981,English\nMr. Turner,2014-01-01,Mike Leigh,7,English\nJimmy's Hall,2014-01-01,Ken Loach,6.69999981,English\nThe Homesman,2014-01-01,Tommy Lee Jones,6.5999999,English\nThe Captive,2014-01-01,Atom Egoyan,5.9000001,english\nGoodbye to Language,2014-01-01,Jean-Luc Godard,6.19999981,French\nThe Search,2014-01-01,Michel Hazanavicius,6.9000001,French\nStill the Water,2014-01-01,Naomi Kawase,6.9000001,Japanese\nMommy,2014-01-01,Xavier Dolan,8.30000019,French Two Days, One Night ,2014-01-01,Jean-Pierre Dardenne,7.4000001,French\nMaps to the Stars,2014-01-01,David Cronenberg,6.4000001,English\nSaint Laurent,2014-01-01,Bertrand Bonello,6.5,French\nClouds of Sils Maria,2014-01-01,Olivier Assayas,6.9000001,english\nWinter Sleep,2014-01-01,Nuri Bilge Ceylan,8.5,Turkish  Finally  /film_nomination :  competition,film,won\n1,1,f\n1,2,f\n1,3,f\n1,4,f\n1,5,f\n1,6,f\n1,7,f\n1,8,f\n1,9,f\n1,10,f\n1,11,f\n1,12,f\n1,13,f\n1,14,f\n1,15,f\n1,16,f\n1,17,f\n1,18,f\n1,19,f\n1,20,f\n2,21,f\n2,22,f\n2,23,f\n2,24,f\n2,25,f\n2,26,f\n2,27,f\n2,28,f\n2,29,f\n2,30,f\n2,31,f\n2,32,f\n2,33,f\n2,34,f\n2,35,f  Getting and Embedding Data  First let's review which films are stored in the database:  GET http://localhost:3000/film  It gives us back a list of JSON objects. What if we care only about the film titles? Use  select  to shape the output:  GET http://localhost:3000/film?select=title  [\n  {\n     title :  Chuang ru zhe \n  },\n  {\n     title :  The Look of Silence \n  },\n  {\n     title :  Fires on the Plain \n  },\n  ...\n]  Here is where it gets cool. PostgREST can embed objects in its response through foreign key relationships. Earlier we created a join table called  film_nomination . It joins films and competitions. We can ask the server about the structure of this table:  OPTIONS http://localhost:3000/film_nomination  {\n   pkey : [\n     id \n  ],\n   columns : [\n    {\n       references : null,\n       default :  nextval('film_nomination_id_seq'::regclass) ,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  id ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 1\n    },\n    {\n       references : {\n         schema :  public ,\n         column :  id ,\n         table :  competition \n      },\n       default : null,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  competition ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 2\n    },\n    {\n       references : {\n         schema :  public ,\n         column :  id ,\n         table :  film \n      },\n       default : null,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  film ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 3\n    },\n    {\n       references : null,\n       default :  true ,\n       precision : null,\n       updatable : true,\n       schema :  public ,\n       name :  won ,\n       type :  boolean ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 4\n    }\n  ]\n}  From this you can see that the columns  film  and  competition  reference their eponymous tables. Let's ask the server for each film along with names of the competitions it entered. You don't have to do any custom coding. Send this query:  GET http://localhost:3000/film?select=title,competition{name}  [\n  {\n     title :  Chuang ru zhe ,\n     competition : [\n      {\n         name :  Golden Lion \n      }\n    ]\n  },\n  {\n     title :  The Look of Silence ,\n     competition : [\n      {\n         name :  Golden Lion \n      }\n    ]\n  },\n  ...\n]  The relation flows both ways. Here is how to get the name of each competition's name and the movies shown at it.  GET http://localhost:3000/competition?select=name,film{title}  [\n  {\n     name :  Golden Lion ,\n     film : [\n      {\n         title :  Chuang ru zhe \n      },\n      {\n         title :  The Look of Silence \n      },\n      ...\n    ]\n  },\n  {\n     name :  Palme d'Or ,\n     film : [\n      {\n         title :  The Wonders \n      },\n      {\n         title :  Foxcatcher \n      },\n      ...\n    ]\n  }\n]  Why not learn about the directors too? There is a many-to-one relation directly between films and directors. We can alter our previous query to include directors in its results.  GET http://localhost:3000/competition?select=name,film{title,director{*}}  [\n  {\n     name :  Golden Lion ,\n     film : [\n      {\n         title :  Manglehorn ,\n         director : {\n           name :  David Gordon Green \n        }\n      },\n      {\n         title :  Belye nochi pochtalona Alekseya Tryapitsyna ,\n         director : {\n           name :  Andrey Konchalovskiy \n        }\n      },\n      ...\n    ]\n  },\n  ...\n]  Singular Responses  How do we ask for a single film, for instance the second one we inserted?  GET http://localhost:3000/film?id=eq.2  It returns  [\n  {\n     id : 2,\n     title :  The Look of Silence ,\n     year :  2014-01-01 ,\n     director :  Joshua Oppenheimer ,\n     rating : 8.3,\n     language :  Indonesian \n  }\n]  Like any query, it gives us a result  set , in this case an array with one element. However you and I know that  id  is a primary key, it will never return more than one result. We might want it returned as a JSON object, not an array. To express this preference include the header  Prefer: plurality=singular . It will respond with  {\n   id : 2,\n   title :  The Look of Silence ,\n   year :  2014-01-01 ,\n   director :  Joshua Oppenheimer ,\n   rating : 8.3,\n   language :  Indonesian \n}  \n     Why this approach to singular responses? \n\n     \n    PostgREST knows which columns comprise a primary key for a\n    table, so why not automatically choose plurality=singular when\n    these column filters are present? The fact is it could come as a\n    shock to a client that by adding one more filter condition it can\n    change the entire response format.\n     \n     \n    Then why not expose another kind of route such as /film/2 to indicate\n    one particular film? Because this does not accommodate compound keys.\n    The convention complects a plurality preference with table key\n    assumptions. We should separate concerns.\n     \n     \n    It turns out you can still have routes like /film/2.  Use a\n    proxy such as Nginx. It can rewrite routes such as /films/2\n    into /films?id=eq.2 and add the Prefer header to make the results\n    singular.\n       Conclusion  This tutorial showed how to create a database with a basic schema, run PostgREST, and interact with the API. The next tutorial will show how to enable security for a multi-tenant blogging API.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/examples/users/", 
            "text": "User Management\n\n\nAPI clients authenticate with \nJSON Web Tokens\n.\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na username and password system on top of JWT using only plpgsql.\n\n\nFuture examples such as the multi-tenant blogging platform will use\nthe results from this example for their auth. We will build a system\nfor users to sign up, log in, manage their accounts, and for admins\nto manange other people's accounts. We will also see how to trigger\noutside events like sending password reset emails.\n\n\nBefore jumping into the code, a little more about how the tokens\nwork. Every JWT contains cryptographically signed \nclaims\n. PostgREST\ncares specificaly about a claim called \nrole\n. When a client includes\na \nrole\n claim PostgREST executes their request using that database\nrole.\n\n\nHow would a client include a role claim, or claims in general?\nWithout knowing the server JWT secret a client cannot create a\nclaim. The only place to get a JWT is from the PostgREST server or\nfrom another service sharing the secret and acting on its behalf.\nWe'll use a stored procedure returning type \njwt_claims\n which is\na special type causing the server to encrypt and sign the return\nvalue.\n\n\nStoring Users and Passwords\n\n\nWe create a database schema especially for auth information. We'll\nalso need the postgres extensions\n\npgcrypto\n and\n\nuuid-ossp\n.\n\n\ncreate extension if not exists pgcrypto;\ncreate extension if not exists \nuuid-ossp\n;\n\n-- We put things inside the basic_auth schema to hide\n-- them from public view. Certain public procs/views will\n-- refer to helpers and tables inside.\ncreate schema if not exists basic_auth;\n\n\n\n\nNext a table to store the mapping from usernames and passwords to\ndatabase roles. The code below includes triggers and functions to\nencrypt the password and ensure the role exists.\n\n\ncreate table if not exists\nbasic_auth.users (\n  email    text primary key check ( email ~* '^.+@.+\\..+$' ),\n  pass     text not null check (length(pass) \n 512),\n  role     name not null check (length(role) \n 512),\n  verified boolean not null default false\n  -- If you like add more columns, or a json column\n);\n\ncreate or replace function\nbasic_auth.check_role_exists() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if not exists (select 1 from pg_roles as r where r.rolname = new.role) then\n    raise foreign_key_violation using message =\n      'unknown database role: ' || new.role;\n    return null;\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists ensure_user_role_exists on basic_auth.users;\ncreate constraint trigger ensure_user_role_exists\n  after insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.check_role_exists();\n\ncreate or replace function\nbasic_auth.encrypt_pass() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if tg_op = 'INSERT' or new.pass \n old.pass then\n    new.pass = crypt(new.pass, gen_salt('bf'));\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists encrypt_pass on basic_auth.users;\ncreate trigger encrypt_pass\n  before insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.encrypt_pass();\n\n\n\n\nWith the table in place we can make a helper to check passwords.\nIt returns the database role for a user if the email and password\nare correct.\n\n\ncreate or replace function\nbasic_auth.user_role(email text, pass text) returns name\n  language plpgsql\n  as $$\nbegin\n  return (\n  select role from basic_auth.users\n   where users.email = user_role.email\n     and users.pass = crypt(user_role.pass, users.pass)\n  );\nend;\n$$;\n\n\n\n\nPassword Reset\n\n\nWhen a user requests a password reset or signs up we create a token\nthey will use later to prove their identity. The tokens go in this\ntable.\n\n\ndrop type if exists token_type_enum cascade;\ncreate type token_type_enum as enum ('validation', 'reset');\n\ncreate table if not exists\nbasic_auth.tokens (\n  token       uuid primary key,\n  token_type  token_type_enum not null,\n  email       text not null references basic_auth.users (email)\n                on delete cascade on update cascade,\n  created_at  timestamptz not null default current_date\n);\n\n\n\n\nIn the main schema (as opposed to the \nbasic_auth\n schema) we expose\na password reset request function. HTTP clients will call it. The\nfunction takes the email address of the user.\n\n\ncreate or replace function\nrequest_password_reset(email text) returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = request_password_reset.email;\n\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', request_password_reset.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', request_password_reset.email,\n      'token', tok,\n      'token_type', 'reset'\n    )::text\n  );\nend;\n$$;\n\n\n\n\nThis function does not send any emails. It sends a postgres\n\nNOTIFY\n\ncommand. External programs such as a mailer listen for this event\nand do the work. The most robust way to process these signals is\nby pushing them onto work queues. Here are two programs to do that:\n\n\n\n\naweber/pgsql-listen-exchange\n for RabbitMQ\n\n\nSpiderOak/skeeter\n for ZeroMQ\n\n\n\n\nFor experimentation you don't need that though. Here's a sample\nNode program that listens for the events and logs them to stdout.\n\n\nvar PS = require('pg-pubsub');\n\nif(process.argv.length !== 3) {\n  console.log(\nUSAGE: DB_URL\n);\n  process.exit(2);\n}\nvar url  = process.argv[2],\n    ps   = new PS(url);\n\n// password reset request events\nps.addChannel('reset', console.log);\n// email validation required event\nps.addChannel('validate', console.log);\n\n// modify me to send emails\n\n\n\n\nOnce the user has a reset token they can use it as an argument to\nthe password reset function, calling it through the PostgREST RPC\ninterface.\n\n\ncreate or replace function\nreset_password(email text, token uuid, pass text)\n  returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  if exists(select 1 from basic_auth.tokens\n             where tokens.email = reset_password.email\n               and tokens.token = reset_password.token\n               and token_type = 'reset') then\n    update basic_auth.users set pass=reset_password.pass\n     where users.email = reset_password.email;\n\n    delete from basic_auth.tokens\n     where tokens.email = reset_password.email\n       and tokens.token = reset_password.token\n       and token_type = 'reset';\n  else\n    raise invalid_password using message =\n      'invalid user or token';\n  end if;\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = reset_password.email;\n\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', reset_password.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', reset_password.email,\n      'token', tok\n    )::text\n  );\nend;\n$$;\n\n\n\n\nEmail Validation\n\n\nThis is similar to password resets. Once again we generate a token.\nIt differs in that there is a trigger to send validations when a\nnew login is added to the users table.\n\n\ncreate or replace function\nbasic_auth.send_validation() returns trigger\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'validation', new.email);\n  perform pg_notify('validate',\n    json_build_object(\n      'email', new.email,\n      'token', tok,\n      'token_type', 'validation'\n    )::text\n  );\n  return new;\nend\n$$;\n\ndrop trigger if exists send_validation on basic_auth.users;\ncreate trigger send_validation\n  after insert on basic_auth.users\n  for each row\n  execute procedure basic_auth.send_validation();\n\n\n\n\nEditing Own User\n\n\nWe'll construct a redacted view for users. It hides passwords and\nshows only those users whose roles the currently logged in user has\ndb permission to access.\n\n\ncreate or replace view users as\nselect actual.role as role,\n       '***'::text as pass,\n       actual.email as email,\n       actual.verified as verified\nfrom basic_auth.users as actual,\n     (select rolname\n        from pg_authid\n       where pg_has_role(current_user, oid, 'member')\n     ) as member_of\nwhere actual.role = member_of.rolname;\n  -- can also add restriction that current_setting('postgrest.claims.email')\n  -- is equal to email so that user can only see themselves\n\n\n\n\nUsing this view clients can see themeslves and any other users with\nthe right db roles. This view does not yet support inserts or updates\nbecause not all the columns refer directly to underlying columns.\nNor do we want it to be auto-updatable because it would allow an escalation\nof privileges. Someone could update their own row and change their\nrole to become more powerful.\n\n\nWe'll handle updates with a trigger, but we'll need a helper function\nto prevent an escalation of privileges.\n\n\ncreate or replace function\nbasic_auth.clearance_for_role(u name) returns void as\n$$\ndeclare\n  ok boolean;\nbegin\n  select exists (\n    select rolname\n      from pg_authid\n     where pg_has_role(current_user, oid, 'member')\n       and rolname = u\n  ) into ok;\n  if not ok then\n    raise invalid_password using message =\n      'current user not member of role ' || u;\n  end if;\nend\n$$ LANGUAGE plpgsql;\n\n\n\n\nWith the above function we can now make a safe trigger to allow\nuser updates.\n\n\ncreate or replace function\nupdate_users() returns trigger\nlanguage plpgsql\nAS $$\nbegin\n  if tg_op = 'INSERT' then\n    perform basic_auth.clearance_for_role(new.role);\n\n    insert into basic_auth.users\n      (role, pass, email, verified)\n    values\n      (new.role, new.pass, new.email,\n      coalesce(new.verified, false));\n    return new;\n  elsif tg_op = 'UPDATE' then\n    -- no need to check clearance for old.role because\n    -- an ineligible row would not have been available to update (http 404)\n    perform basic_auth.clearance_for_role(new.role);\n\n    update basic_auth.users set\n      email  = new.email,\n      role   = new.role,\n      pass   = new.pass,\n      verified = coalesce(new.verified, old.verified, false)\n      where email = old.email;\n    return new;\n  elsif tg_op = 'DELETE' then\n    -- no need to check clearance for old.role (see previous case)\n\n    delete from basic_auth.users\n     where basic_auth.email = old.email;\n    return null;\n  end if;\nend\n$$;\n\ndrop trigger if exists update_users on users;\ncreate trigger update_users\n  instead of insert or update or delete on\n    users for each row execute procedure update_users();\n\n\n\n\nFinally add a public function people can use to sign up. You can\nhard code a default db role in it. It alters the underlying\n\nbasic_auth.users\n so you can set whatever role you want without\nrestriction.\n\n\ncreate or replace function\nsignup(email text, pass text) returns void\nas $$\n  insert into basic_auth.users (email, pass, role) values\n    (signup.email, signup.pass, 'hardcoded-role-here');\n$$ language sql;\n\n\n\n\nGenerating JWT\n\n\nAs mentioned at the start, clients authenticate with JWT. PostgREST\nhas a special convention to allow your sql functions to return JWT.\nAny function that returns a type whose name ends in \njwt_claims\n will\nhave its return value encoded. For instance, let's make a login function\nwhich consults our users table.\n\n\nFirst create a return type:\n\n\ndrop type if exists basic_auth.jwt_claims cascade;\ncreate type basic_auth.jwt_claims AS (role text, email text);\n\n\n\n\nAnd now the function:\n\n\ncreate or replace function\nlogin(email text, pass text) returns basic_auth.jwt_claims\n  language plpgsql\n  as $$\ndeclare\n  _role name;\n  result basic_auth.jwt_claims;\nbegin\n  select basic_auth.user_role(email, pass) into _role;\n  if _role is null then\n    raise invalid_password using message = 'invalid user or password';\n  end if;\n  -- TODO; check verified flag if you care whether users\n  -- have validated their emails\n  select _role as role, login.email as email into result;\n  return result;\nend;\n$$;\n\n\n\n\nAn API request to login would look like this.\n\n\nPOST /rpc/login\n\n{ \nemail\n: \nfoo@bar.com\n, \npass\n: \nfoobar\n }\n\n\n\n\nResponse\n\n\n{\n  \ntoken\n: \neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto\n\n}\n\n\n\n\nTry decoding the token at \njwt.io\n. (It was encoded\nwith a secret of \nsecret\n which is the default.) To use this token\nin a future API request include it in an \nAuthorization\n request\nheader.\n\n\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto\n\n\n\n\nConclusion\n\n\nThis section explained the implementation details for building a\npassword based authentication system in pure sql. The next example\nwill put it to work in a multi-tenant blogging API.", 
            "title": "User Management"
        }, 
        {
            "location": "/examples/users/#user-management", 
            "text": "API clients authenticate with  JSON Web Tokens .\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na username and password system on top of JWT using only plpgsql.  Future examples such as the multi-tenant blogging platform will use\nthe results from this example for their auth. We will build a system\nfor users to sign up, log in, manage their accounts, and for admins\nto manange other people's accounts. We will also see how to trigger\noutside events like sending password reset emails.  Before jumping into the code, a little more about how the tokens\nwork. Every JWT contains cryptographically signed  claims . PostgREST\ncares specificaly about a claim called  role . When a client includes\na  role  claim PostgREST executes their request using that database\nrole.  How would a client include a role claim, or claims in general?\nWithout knowing the server JWT secret a client cannot create a\nclaim. The only place to get a JWT is from the PostgREST server or\nfrom another service sharing the secret and acting on its behalf.\nWe'll use a stored procedure returning type  jwt_claims  which is\na special type causing the server to encrypt and sign the return\nvalue.  Storing Users and Passwords  We create a database schema especially for auth information. We'll\nalso need the postgres extensions pgcrypto  and uuid-ossp .  create extension if not exists pgcrypto;\ncreate extension if not exists  uuid-ossp ;\n\n-- We put things inside the basic_auth schema to hide\n-- them from public view. Certain public procs/views will\n-- refer to helpers and tables inside.\ncreate schema if not exists basic_auth;  Next a table to store the mapping from usernames and passwords to\ndatabase roles. The code below includes triggers and functions to\nencrypt the password and ensure the role exists.  create table if not exists\nbasic_auth.users (\n  email    text primary key check ( email ~* '^.+@.+\\..+$' ),\n  pass     text not null check (length(pass)   512),\n  role     name not null check (length(role)   512),\n  verified boolean not null default false\n  -- If you like add more columns, or a json column\n);\n\ncreate or replace function\nbasic_auth.check_role_exists() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if not exists (select 1 from pg_roles as r where r.rolname = new.role) then\n    raise foreign_key_violation using message =\n      'unknown database role: ' || new.role;\n    return null;\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists ensure_user_role_exists on basic_auth.users;\ncreate constraint trigger ensure_user_role_exists\n  after insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.check_role_exists();\n\ncreate or replace function\nbasic_auth.encrypt_pass() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if tg_op = 'INSERT' or new.pass   old.pass then\n    new.pass = crypt(new.pass, gen_salt('bf'));\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists encrypt_pass on basic_auth.users;\ncreate trigger encrypt_pass\n  before insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.encrypt_pass();  With the table in place we can make a helper to check passwords.\nIt returns the database role for a user if the email and password\nare correct.  create or replace function\nbasic_auth.user_role(email text, pass text) returns name\n  language plpgsql\n  as $$\nbegin\n  return (\n  select role from basic_auth.users\n   where users.email = user_role.email\n     and users.pass = crypt(user_role.pass, users.pass)\n  );\nend;\n$$;  Password Reset  When a user requests a password reset or signs up we create a token\nthey will use later to prove their identity. The tokens go in this\ntable.  drop type if exists token_type_enum cascade;\ncreate type token_type_enum as enum ('validation', 'reset');\n\ncreate table if not exists\nbasic_auth.tokens (\n  token       uuid primary key,\n  token_type  token_type_enum not null,\n  email       text not null references basic_auth.users (email)\n                on delete cascade on update cascade,\n  created_at  timestamptz not null default current_date\n);  In the main schema (as opposed to the  basic_auth  schema) we expose\na password reset request function. HTTP clients will call it. The\nfunction takes the email address of the user.  create or replace function\nrequest_password_reset(email text) returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = request_password_reset.email;\n\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', request_password_reset.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', request_password_reset.email,\n      'token', tok,\n      'token_type', 'reset'\n    )::text\n  );\nend;\n$$;  This function does not send any emails. It sends a postgres NOTIFY \ncommand. External programs such as a mailer listen for this event\nand do the work. The most robust way to process these signals is\nby pushing them onto work queues. Here are two programs to do that:   aweber/pgsql-listen-exchange  for RabbitMQ  SpiderOak/skeeter  for ZeroMQ   For experimentation you don't need that though. Here's a sample\nNode program that listens for the events and logs them to stdout.  var PS = require('pg-pubsub');\n\nif(process.argv.length !== 3) {\n  console.log( USAGE: DB_URL );\n  process.exit(2);\n}\nvar url  = process.argv[2],\n    ps   = new PS(url);\n\n// password reset request events\nps.addChannel('reset', console.log);\n// email validation required event\nps.addChannel('validate', console.log);\n\n// modify me to send emails  Once the user has a reset token they can use it as an argument to\nthe password reset function, calling it through the PostgREST RPC\ninterface.  create or replace function\nreset_password(email text, token uuid, pass text)\n  returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  if exists(select 1 from basic_auth.tokens\n             where tokens.email = reset_password.email\n               and tokens.token = reset_password.token\n               and token_type = 'reset') then\n    update basic_auth.users set pass=reset_password.pass\n     where users.email = reset_password.email;\n\n    delete from basic_auth.tokens\n     where tokens.email = reset_password.email\n       and tokens.token = reset_password.token\n       and token_type = 'reset';\n  else\n    raise invalid_password using message =\n      'invalid user or token';\n  end if;\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = reset_password.email;\n\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', reset_password.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', reset_password.email,\n      'token', tok\n    )::text\n  );\nend;\n$$;  Email Validation  This is similar to password resets. Once again we generate a token.\nIt differs in that there is a trigger to send validations when a\nnew login is added to the users table.  create or replace function\nbasic_auth.send_validation() returns trigger\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  select uuid_generate_v4() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'validation', new.email);\n  perform pg_notify('validate',\n    json_build_object(\n      'email', new.email,\n      'token', tok,\n      'token_type', 'validation'\n    )::text\n  );\n  return new;\nend\n$$;\n\ndrop trigger if exists send_validation on basic_auth.users;\ncreate trigger send_validation\n  after insert on basic_auth.users\n  for each row\n  execute procedure basic_auth.send_validation();  Editing Own User  We'll construct a redacted view for users. It hides passwords and\nshows only those users whose roles the currently logged in user has\ndb permission to access.  create or replace view users as\nselect actual.role as role,\n       '***'::text as pass,\n       actual.email as email,\n       actual.verified as verified\nfrom basic_auth.users as actual,\n     (select rolname\n        from pg_authid\n       where pg_has_role(current_user, oid, 'member')\n     ) as member_of\nwhere actual.role = member_of.rolname;\n  -- can also add restriction that current_setting('postgrest.claims.email')\n  -- is equal to email so that user can only see themselves  Using this view clients can see themeslves and any other users with\nthe right db roles. This view does not yet support inserts or updates\nbecause not all the columns refer directly to underlying columns.\nNor do we want it to be auto-updatable because it would allow an escalation\nof privileges. Someone could update their own row and change their\nrole to become more powerful.  We'll handle updates with a trigger, but we'll need a helper function\nto prevent an escalation of privileges.  create or replace function\nbasic_auth.clearance_for_role(u name) returns void as\n$$\ndeclare\n  ok boolean;\nbegin\n  select exists (\n    select rolname\n      from pg_authid\n     where pg_has_role(current_user, oid, 'member')\n       and rolname = u\n  ) into ok;\n  if not ok then\n    raise invalid_password using message =\n      'current user not member of role ' || u;\n  end if;\nend\n$$ LANGUAGE plpgsql;  With the above function we can now make a safe trigger to allow\nuser updates.  create or replace function\nupdate_users() returns trigger\nlanguage plpgsql\nAS $$\nbegin\n  if tg_op = 'INSERT' then\n    perform basic_auth.clearance_for_role(new.role);\n\n    insert into basic_auth.users\n      (role, pass, email, verified)\n    values\n      (new.role, new.pass, new.email,\n      coalesce(new.verified, false));\n    return new;\n  elsif tg_op = 'UPDATE' then\n    -- no need to check clearance for old.role because\n    -- an ineligible row would not have been available to update (http 404)\n    perform basic_auth.clearance_for_role(new.role);\n\n    update basic_auth.users set\n      email  = new.email,\n      role   = new.role,\n      pass   = new.pass,\n      verified = coalesce(new.verified, old.verified, false)\n      where email = old.email;\n    return new;\n  elsif tg_op = 'DELETE' then\n    -- no need to check clearance for old.role (see previous case)\n\n    delete from basic_auth.users\n     where basic_auth.email = old.email;\n    return null;\n  end if;\nend\n$$;\n\ndrop trigger if exists update_users on users;\ncreate trigger update_users\n  instead of insert or update or delete on\n    users for each row execute procedure update_users();  Finally add a public function people can use to sign up. You can\nhard code a default db role in it. It alters the underlying basic_auth.users  so you can set whatever role you want without\nrestriction.  create or replace function\nsignup(email text, pass text) returns void\nas $$\n  insert into basic_auth.users (email, pass, role) values\n    (signup.email, signup.pass, 'hardcoded-role-here');\n$$ language sql;  Generating JWT  As mentioned at the start, clients authenticate with JWT. PostgREST\nhas a special convention to allow your sql functions to return JWT.\nAny function that returns a type whose name ends in  jwt_claims  will\nhave its return value encoded. For instance, let's make a login function\nwhich consults our users table.  First create a return type:  drop type if exists basic_auth.jwt_claims cascade;\ncreate type basic_auth.jwt_claims AS (role text, email text);  And now the function:  create or replace function\nlogin(email text, pass text) returns basic_auth.jwt_claims\n  language plpgsql\n  as $$\ndeclare\n  _role name;\n  result basic_auth.jwt_claims;\nbegin\n  select basic_auth.user_role(email, pass) into _role;\n  if _role is null then\n    raise invalid_password using message = 'invalid user or password';\n  end if;\n  -- TODO; check verified flag if you care whether users\n  -- have validated their emails\n  select _role as role, login.email as email into result;\n  return result;\nend;\n$$;  An API request to login would look like this.  POST /rpc/login\n\n{  email :  foo@bar.com ,  pass :  foobar  }  Response  {\n   token :  eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto \n}  Try decoding the token at  jwt.io . (It was encoded\nwith a secret of  secret  which is the default.) To use this token\nin a future API request include it in an  Authorization  request\nheader.  Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto  Conclusion  This section explained the implementation details for building a\npassword based authentication system in pure sql. The next example\nwill put it to work in a multi-tenant blogging API.", 
            "title": "User Management"
        }
    ]
}